{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision\n",
    "\n",
    "Let's do some very basic computer vision. We're going to import the MNIST handwritten digits data and $k$NN to predict values (i.e. \"see/read\").\n",
    "\n",
    "1. To load the data, run the following code in a chunk:\n",
    "```\n",
    "from keras.datasets import mnist\n",
    "df = mnist.load_data('minst.db')\n",
    "train,test = df\n",
    "X_train, y_train = train\n",
    "X_test, y_test = test\n",
    "```\n",
    "The `y_test` and `y_train` vectors, for each index `i`, tell you want number is written in the corresponding index in `X_train[i]` and `X_test[i]`. The value of `X_train[i]` and `X_test[i]`, however, is a 28$\\times$28 array whose entries contain values between 0 and 256. Each element of the matrix is essentially a \"pixel\" and the matrix encodes a representation of a number. To visualize this, run the following code to see the first ten numbers:\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000)\n",
    "for i in range(5): \n",
    "    print(y_test[i],'\\n') # Print the label\n",
    "    print(X_test[i],'\\n') # Print the matrix of values\n",
    "    plt.contourf(np.rot90(X_test[i].transpose())) # Make a contour plot of the matrix values\n",
    "    plt.show()\n",
    "```\n",
    "OK, those are the data: Labels attached to handwritten digits encoded as a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "df = mnist.load_data('minst.db')\n",
    "train,test = df\n",
    "X_train, y_train = train\n",
    "X_test, y_test = test\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000)\n",
    "for i in range(5): \n",
    "    print(y_test[i],'\\n') # Print the label\n",
    "    print(X_test[i],'\\n') # Print the matrix of values\n",
    "    plt.contourf(np.rot90(X_test[i].transpose())) # Make a contour plot of the matrix values\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the shape of `X_train` and `X_test`? What is the shape of `X_train[i]` and `X_test[i]` for each index `i`? What is the shape of `y_train` and `y_test`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train[0].shape)\n",
    "print(X_test[0].shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are 60,000 matrices of size 28 by 28 in the training set, and 10,000 matrices of size 28 by 28 in the test set. The y_train vector has 60,000 numeral assignments, and the y_test vector has 10,000 numeral assignments. Basically, each `X_train[i]` is a matrix of values in two-dimensional space, associated with a numeral in `y_train[i]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use Numpy's `.reshape()` method to covert the training and testing data from a matrix into an vector of features. So, `X_test[index].reshape((1,784))` will convert the $index$-th element of `X_test` into a $28\\times 28=784$-length row vector of values, rather than a matrix. Turn `X_train` into an $N \\times 784$ matrix $X$ that is suitable for scikit-learn's kNN classifier where $N$ is the number of observations and $784=28*28$ (you could use, for example, a `for` loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To save on reloading cost, I save the reshaped data and reload it rather than run the\n",
    "# code that loops over appending the rows \n",
    "\n",
    "reload = 0 # Control the way data loads\n",
    "\n",
    "if reload == 1:  # If reload is 1, do the reshaping process\n",
    "    Z_train = []\n",
    "    for i in range(60000):\n",
    "        row = X_train[i].reshape((1,784)) # Turn the matrix for i into a row vector of features\n",
    "        Z_train.append(row[0]) # Append the row vector to the list\n",
    "    Z_train = pd.DataFrame(Z_train)\n",
    "    Z_train.to_csv('/Users/vaibhavjha/labsvj/03_computer_vision/Z_train.csv')\n",
    "    #\n",
    "    Z_test = []\n",
    "    for i in range(len(y_test)):\n",
    "        row = X_test[i].reshape((1,784)) # Turn the matrix for i into a row vector of features\n",
    "        Z_test.append(row[0]) # Append the row vector to the list\n",
    "    Z_test = pd.DataFrame(Z_test)\n",
    "    Z_test.to_csv('/Users/vaibhavjha/labsvj/03_computer_vision/Z_train.csv')\n",
    "else: # If reload is not 1, just load the reshaped data\n",
    "    Z_train = pd.read_csv('/Users/vaibhavjha/labsvj/03_computer_vision/Z_train.csv')\n",
    "    Z_test = pd.read_csv('/Users/vaibhavjha/labsvj/03_computer_vision/Z_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use the reshaped `X_test` and `y_test` data to create a $k$-nearest neighbor classifier of digit. What is the optimal number of neighbors $k$? If you can't determine this, play around with different values of $k$ for your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Determine the optimal k:\n",
    "k_bar = 50\n",
    "k_grid = np.arange(2,k_bar) # The range of k's to consider\n",
    "accuracy = np.zeros(k_bar) \n",
    "\n",
    "for k in range(k_bar):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k+1)\n",
    "    predictor = knn.fit(Z_train.values,y_train) \n",
    "    #y_hat = predictor.predict(Z_test.values) \n",
    "    accuracy[k] = knn.score(Z_test.values,y_test) # Bug in sklearn requires .values\n",
    "\n",
    "accuracy_max = np.max(accuracy) # highest recorded accuracy\n",
    "max_index = np.where(accuracy==accuracy_max) \n",
    "k_star = k_grid[max_index] # Find the optimal value of k\n",
    "print(k_star)\n",
    "\n",
    "plt.plot(np.arange(0,k_bar),accuracy) # Plot accuracy by k\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"optimal k:\"+str(k_star))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. For the optimal number of neighbors, how well does your predictor perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "predictor = knn.fit(Z_train.values,y_train) \n",
    "y_hat = predictor.predict(Z_test.values) \n",
    "\n",
    "accuracy = knn.score(Z_test.values,y_test) # Bug in sklearn requires .values\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "pd.crosstab(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With k=3, the rule is 90% accurate on the test set. When it does make mistakes, it tends to be things like confusing 4 for 9 or 8 for 3 or 7 for 1, which is understandable. It is remarkable that a simple algorithm like kNN does this well at classifying such complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For your confusion matrix, which mistakes are most likely? Do you find any interesting patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The biggest mistakes are mistaking a 7 for 1 (39), a 9 for a 7 (40), an 8 for a 3 (50), a 9 for a 4 (51), and a 8 for a 5 (45), a 4 for a 9 (81). The pattern here is that these are all very visually similar, so it makes sense that a computer would make these mistakes, since even humans make these mistakes sometimes, especially when the written value isn't very legible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. So, this is how computers \"see.\" They convert an image into a matrix of values, that matrix becomes a vector in a dataset, and then we deploy ML tools on it as if it was any other kind of tabular data. To make sure you follow this, invent a way to represent a color photo in matrix form, and then describe how you could convert it into tabular data. (Hint: RGB color codes provide a method of encoding a numeric value that represents a color.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The current data include an \"intensity\" for each pixel in the 28$\\times$28 grid. To add color, we could have three $28 \\times 28$ matrices that each capture the Red, Green, or Blue color intensity. Then we would reshape the three matrices and put them side by side into one long row to create tabular data, like we did above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
